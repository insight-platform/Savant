# module name, required
name: ${oc.env:MODULE_NAME, 'demo'}

# base module parameters
parameters:
  # pipeline processing frame parameters
  frame:
    width: 1280
    height: 720
    # Add paddings to the frame before processing
    padding:
      # Paddings are kept on the output frame
      keep: true
      left: 0
      right: 0
      # Padding size is 180px on the top of the frame
      top: 180
      bottom: 0
    geometry_base: 4
  output_frame:
    # Frame is output without any encoding
    # this is to circumvent 3 hardware decoding processes limit on NVIDIA consumer hardware
    codec: raw-rgba
  # PyFunc for drawing on frames
  draw_func:
    # specify the drawfunc's python module
    module: samples.peoplenet_detector.overlay
    # specify the drawfunc's python class from the module
    class_name: Overlay
    # drawfunc's class init keyword arguments
    # will be available as drawfunc object's attributes
    kwargs:
      # height of overlay dashboard in pixels
      overlay_height: 180
      # height of logo image in pixels
      logo_height: 120
      # the height of animation's sprites in pixels
      sprite_height: 120
      # height of counter's numbers in pixels
      counters_height: 85
      # thickness of counter's font
      counters_font_thickness: 5
    rendered_objects:
      # Detector's element name from pipeline definition below
      peoplenet:
        # Detector's output object's label from pipeline definition
        # Note that in this sample the analytics pyfunc assigns draw labels to objects
        # If assigned, draw labels are used in place of the original object labels
        # in the default draw func implementation
        person_face:
          bbox:
            # RGBA color value for bboxes of persons with face associated
            border_color: '00ff00ff'
            # width of person's bboxes border in pixels
            thickness: 3
          label:
            # RGBA color values for person label's font and background
            font_color: '000000ff'
            background_color: 'd8e6ffd9'
        person_noface:
          bbox:
            # RGBA color value for bboxes of persons with no face associated
            border_color: '0000ffff'
            # width of person's bboxes border in pixels
            thickness: 3
          label:
            # RGBA color values for person label's font and background
            font_color: '000000ff'
            background_color: 'd8e6ffd9'
        face:
          blur: true

# pipeline definition
pipeline:
  # source definition is skipped, zeromq source is used by default to connect with source adapters

  # define pipeline's main elements
  elements:
    # primary detector element, inference is provided by the nvinfer Deepstream element
    # model type is detector (other available types are: classifier, custom)
    - element: nvinfer@detector
      # Model's name in the pipeline, mandatory
      name: peoplenet
      # model definition
      model:
        # format of the provided model file
        format: etlt
        # remote storage where the model files can be found
        # skip if providing model files locally
        remote:
          url: s3://savant-data/models/peoplenet/peoplenet_pruned_v2.0.zip
          checksum_url: s3://savant-data/models/peoplenet/peoplenet_pruned_v2.0.md5
          parameters:
            endpoint: https://eu-central-1.linodeobjects.com
          # or get the model directly from NGC API
          # peoplenet v2.0
          # url: "https://api.ngc.nvidia.com/v2/models/nvidia/tao/peoplenet/versions/pruned_v2.0/zip"

        # model file name, without location
        model_file: resnet34_peoplenet_pruned.etlt  # v2.0 Accuracy: 84.3 Size 20.9 MB

        # configuration of input data and custom preprocessing methods
        input:
          # model input layer name
          layer_name: input_1
          # model input layer shape
          shape: [3, 544, 960]
          # pixel scaling/normalization factor
          scale_factor: 0.0039215697906911373

        # configuration of model output
        output:
          # model output layer names
          layer_names: [output_bbox/BiasAdd, output_cov/Sigmoid]
          # number of detected classes for detector model
          num_detected_classes: 3
          # specify which detected objects are included in output
          objects:
            # object class id
            - class_id: 0
              # label assigned to objects of this class id
              label: person
              selector:
                kwargs:
                  # minimal width of the objects of this class to be included in output
                  min_width: 32
                  # minimal height of the objects of this class to be included in output
                  min_height: 32
            - class_id: 2
              label: face
              selector:
                kwargs:
                  # minimal detector confidence of the objects of this class to be included in output
                  confidence_threshold: 0.1

    # nvtracker element from Deepstream, provides tracking
    - element: nvtracker
      properties:
        tracker-width: 640
        tracker-height: 384
        ll-lib-file: /opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so
        # specify tracking config file
        ll-config-file: ${oc.env:PROJECT_PATH}/samples/peoplenet_detector/config_tracker_NvSORT.yml

    # analytics element realized in custom pyfunc
    - element: pyfunc
      # specify the pyfunc's python module
      module: samples.peoplenet_detector.analytics
      # specify the pyfunc's python class from the module
      class_name: Analytics
      # pyfunc's class init keyword arguments
      # will be available as pyfunc object's attributes
      kwargs:
        counters_smoothing_period: 0.25

  # sink definition is skipped, zeromq sink is used by default to connect with sink adapters
